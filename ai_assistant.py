import os
import yaml
import logging
import time
import wave

from audio import initialize_porcupine, capture_wake_word, record_with_vad
from whisper import transcribe_audio
from llama_cpp import Llama

# --- Nastaven칤 logov치n칤 ---
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def load_config():
    """Na캜te konfigura캜n칤 soubor config.yaml."""
    try:
        config_path = os.path.expanduser("~/ai-assistant/config.yaml")
        with open(config_path, "r") as f:
            return yaml.safe_load(f)
    except Exception as e:
        logging.error(f"Chyba p콏i na캜칤t치n칤 konfigurace: {e}")
        exit(1)

def initialize_llama(config):
    model_path = config['llama']['model']
    max_tokens = config['llama'].get('max_tokens', 128)
    llm = Llama(model_path=model_path)
    return llm, max_tokens

def generate_response(llm, prompt: str, max_tokens: int) -> str:
    """Generuje odpov캩캞 pomoc칤 Llama modelu."""
    response = llm(
        prompt=prompt,
        max_tokens=max_tokens,
        temperature=0.7,
        stop=None,
        echo=False,
    )
    return response['choices'][0]['text']

def save_audio(audio_data, config, filename="command.wav"):
    with wave.open(filename, 'wb') as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)  # 16 bit = 2 bytes
        wf.setframerate(config["audio"]["rate"])
        wf.writeframes(audio_data.tobytes())
    logging.info(f"Audio ulo쬰no do {filename}")

def ai_assistant_loop():
    config = load_config()

    porcupine = initialize_porcupine(config)
    llm, max_tokens = initialize_llama(config)

    logging.info("Asistent spu코t캩n, 캜ek치m na kl칤캜ov칠 slovo...")

    while True:
        result = capture_wake_word(porcupine, config)
        if result >= 0:
            logging.info("游릭 Kl칤캜ov칠 slovo detekov치no!")

            logging.info("Nahr치v치m tv콢j p콏칤kaz...")
            audio_data = record_with_vad(config)

            save_audio(audio_data, config)

            text = transcribe_audio("command.wav", config)
            logging.info(f"P콏epsan칳 text: {text}")

            if text.strip():
                logging.info("Generuji odpov캩캞 modelem Llama...")
                odpoved = generate_response(llm, text, max_tokens)
                logging.info(f"Odpov캩캞 asistenta: {odpoved}")

                # TTS, nebo dal코칤 akce m콢쬰코 p콏idat zde
                print("Asistent 콏칤k치:", odpoved)

        time.sleep(0.1)

if __name__ == "__main__":
    ai_assistant_loop()
